
## Point Cloud Interpolation Based on Deep Learning

## Abstract

Among many 3D computer vision tasks, point cloud interpolation is very important since a variety of recognition tasks benefit from more extended input of sequence data. However, existing research mainly focuses on static or single frame point cloud tasks, such as segmentation, classification,  reconstruction, registration, detection, etc. Point cloud interpolation gains less popularity and remains the major challenge because of its high dimensionality and disordered nature. Recently, several works have presented some methods for point cloud interpolation. Thus, particularly prominent in 3D point cloud interpolation, existing models fail to capture the natural shapes. This study will design an efficient and accurate method for point cloud interpolation via deep learning on the dynamic sequences by employing existing techniques and frameworks. We address this challenge by applying a deep neural network architecture. Specifically, we first exploit a neural network on dynamic point cloud sequences, and append temporal dimension to generate spatiotemporal neighborhoods for each point. The extracted geometric features and the dynamic displacements of point clouds under time change are input to estimate scene flow between adjacent point clouds. The intermediate point cloud between two input point clouds is generated by scene flow estimation. Then we import the features into the self-attention module for better feature fusion to improve accuracy. After that, a loss function is constructed to evaluate the accuracy of our results. 

## Installation

Please create a new conda environment by run the command `conda env create -f req.yml` and activate it by `conda activate wyn_pytorch`.
If pip install cuda failed, please run `pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html`.
If the code still fails to run, please check the installed package by

```
pip3 install opencv-python
pip3 install mayavi
pip3 install pyqt5
pip3 install quaternion
pip3 install scikit-learn
pip3 install -U scikit-learn scipy matplotlib
```

## Scene Flow Experiment on KITTI

For the scene flow estimation, please refer to <a href="https://github.com/xingyul/meteornet">MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences</a>. If you want to get the scene flow data fast, follow these steps:

### Data Download

Download KITTI <a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php">scene flow data</a> and <a href="http://www.cvlibs.net/datasets/kitti/raw_data.php">raw data</a>. Extract the `.zip` files such that the scene flow data directory and raw data directory looks like this

use the link: `https://s3.eu-central-1.amazonaws.com/avg-kitti/data_scene_flow.zip` and `https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_scene_flow.zip`

```
/path/to/sceneflow/
    devkit/
    testing/
    training/

/path/to/raw/
    2011_09_26/
    2011_09_29/
    2011_10_03/
    2011_09_28/
    2011_09_30/
```

### Data Processing

The script for processing of data is `gen_kitti_flow.py`. We extract four consecutive frames and generate the scene flow between the from the first to the second frame. It can be run by the following command

```
python gen_kitti_flow.py --kitti_sceneflow_dir /path/to/sceneflow --kitti_raw_dir /path/to/raw --output_dir output_folder


python gen_kitti_flow.py --kitti_sceneflow_dir E:\fyp\dataset\data_scene_flow --kitti_raw_dir E:\fyp\dataset\raw\data --output_dir E:\fyp\dataset\output
```
## Self Attention Fusion Experiment based on DCP

We propose a point fusion method based on point cloud registration. First, register two point cloud frames to the same coordinate system using the point cloud registration method. By doing so, a rigid transformation from coordinate system A to coordinate system B is built. After the registration, we can obtain a rigid transformation matrix. The interpolation operations are performed on this rigid transformation matrix's scaling, rotation, and translation components to generate new scaling, rotation, and translation components. Finally, the new vector obtained after the interpolation operations and the input point cloud are integrated into the final intermediate point cloud.

We choose the <a href="https://github.com/WangYueFt/dcp">Deep Closest Point (DCP)</a> method to do the registration of the two point cloud frames. 

Please run the `eval.py ` to generate the fused point cloud and evaluate its accuracy.

## Visualization

Please run the `vis.py ` to visualize the point clouds.